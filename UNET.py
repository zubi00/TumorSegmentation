import osimport randomfrom pathlib import Pathimport numpy as npimport torchimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import Dataset, DataLoaderimport pytorch_lightning as plfrom pytorch_lightning.callbacks import ModelCheckpointfrom pytorch_lightning.loggers import TensorBoardLoggerimport matplotlib.pyplot as pltfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_scoreimport seaborn as snsclass LungTumorDataset(Dataset):    def __init__(self, data_dir, transform=None):        self.data_dir = Path(data_dir)        self.ct_slices = sorted(list(self.data_dir.glob("ct_slice_*.npy")))        self.masks = sorted(list(self.data_dir.glob("mask_mask_*.npy")))        self.transform = transform    def __len__(self):        return len(self.ct_slices)    def __getitem__(self, idx):        ct_slice = np.load(self.ct_slices[idx])        mask = np.load(self.masks[idx])                ct_slice = torch.from_numpy(ct_slice).float().unsqueeze(0)        mask = torch.from_numpy(mask).long()                if self.transform:            ct_slice = self.transform(ct_slice)                return ct_slice, maskclass DoubleConv(nn.Module):    def __init__(self, in_channels, out_channels):        super(DoubleConv, self).__init__()        self.conv = nn.Sequential(            nn.Conv2d(in_channels, out_channels, 3, padding=1),            nn.BatchNorm2d(out_channels),            nn.ReLU(inplace=True),            nn.Conv2d(out_channels, out_channels, 3, padding=1),            nn.BatchNorm2d(out_channels),            nn.ReLU(inplace=True)        )    def forward(self, x):        return self.conv(x)class UNet(nn.Module):    def __init__(self, in_channels=1, out_channels=2, features=[64, 128, 256, 512]):        super(UNet, self).__init__()        self.encoder = nn.ModuleList()        self.decoder = nn.ModuleList()        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)        # Encoder        for feature in features:            self.encoder.append(DoubleConv(in_channels, feature))            in_channels = feature        # Decoder        for feature in reversed(features):            self.decoder.append(                nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)            )            self.decoder.append(DoubleConv(feature*2, feature))        self.bottleneck = DoubleConv(features[-1], features[-1]*2)        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)    def forward(self, x):        skip_connections = []        # Encoder        for enc in self.encoder:            x = enc(x)            skip_connections.append(x)            x = self.pool(x)        x = self.bottleneck(x)        skip_connections = skip_connections[::-1]        # Decoder        for idx in range(0, len(self.decoder), 2):            x = self.decoder[idx](x)            skip_connection = skip_connections[idx//2]                        if x.shape != skip_connection.shape:                x = torch.nn.functional.resize(x, size=skip_connection.shape[2:])            concat_skip = torch.cat((skip_connection, x), dim=1)            x = self.decoder[idx+1](concat_skip)        return self.final_conv(x)class UNetLightning(pl.LightningModule):    def __init__(self, in_channels=1, out_channels=2, learning_rate=1e-3):        super().__init__()        self.model = UNet(in_channels, out_channels)        self.learning_rate = learning_rate        self.criterion = nn.CrossEntropyLoss()    def forward(self, x):        return self.model(x)    def training_step(self, batch, batch_idx):        x, y = batch        y_hat = self(x)        loss = self.criterion(y_hat, y)        self.log('train_loss', loss)        return loss    def validation_step(self, batch, batch_idx):        x, y = batch        y_hat = self(x)        loss = self.criterion(y_hat, y)        self.log('val_loss', loss)        return loss    def configure_optimizers(self):        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)        return optimizerdef dice_coefficient(pred, target):    smooth = 1e-5    num = pred.size(0)    pred = pred.view(num, -1)    target = target.view(num, -1)    intersection = (pred * target).sum(1)    union = pred.sum(1) + target.sum(1)    dice = (2. * intersection + smooth) / (union + smooth)    return dice.mean()def iou_score(pred, target, threshold=0.5):    pred = (pred > threshold).float()    intersection = (pred * target).sum()    union = pred.sum() + target.sum() - intersection    return (intersection + 1e-5) / (union + 1e-5)def compute_metrics(all_preds, all_masks, thresholds=[0.5, 0.6, 0.7, 0.8, 0.9]):    metrics = {}    metrics['accuracy'] = accuracy_score(all_masks, all_preds)    metrics['sensitivity'] = recall_score(all_masks, all_preds, average='binary', pos_label=1)    metrics['specificity'] = recall_score(all_masks, all_preds, average='binary', pos_label=0)    metrics['precision'] = precision_score(all_masks, all_preds, average='binary')    metrics['recall'] = recall_score(all_masks, all_preds, average='binary')    metrics['f1_score'] = f1_score(all_masks, all_preds, average='binary')    metrics['dcs'] = dice_coefficient(torch.tensor(all_preds), torch.tensor(all_masks))        iou_scores = []    for threshold in thresholds:        iou = iou_score(torch.tensor(all_preds), torch.tensor(all_masks), threshold)        iou_scores.append(iou.item())    metrics['iou_scores'] = iou_scores        return metricsdef main():    # Set random seed for reproducibility    pl.seed_everything(42)    # Define data paths    train_dir = Path("/Users/zubairsaeed/Downloads/PhD/Code/1.Segmentation/Data/Dataset/Preprocessed/segmentation_data/train")    val_dir = Path("/Users/zubairsaeed/Downloads/PhD/Code/1.Segmentation/Data/Dataset/Preprocessed/segmentation_data/val")    # Create datasets    train_dataset = LungTumorDataset(train_dir)    val_dataset = LungTumorDataset(val_dir)    # Create data loaders    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=1, persistent_workers=True)    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=1, persistent_workers=True)    # Initialize model    model = UNetLightning()    # Define callbacks    checkpoint_callback = ModelCheckpoint(        dirpath='checkpoints',        filename='unet-{epoch:02d}-{val_loss:.2f}',        save_top_k=3,        monitor='val_loss',        mode='min'    )    # Define logger    logger = TensorBoardLogger("lightning_logs", name="unet")    # Initialize trainer    device = torch.device("mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu")    trainer = pl.Trainer(        max_epochs=100,        callbacks=[checkpoint_callback],        logger=logger,        accelerator='auto',        devices=1 if device.type in ['cuda', 'mps'] else None    )    # Train the model    trainer.fit(model, train_loader, val_loader)    # Evaluate on validation set    model.eval()    val_dice_score = 0    all_preds = []    all_masks = []    all_confidences = []    with torch.no_grad():        for ct_slices, masks in val_loader:            ct_slices, masks = ct_slices.to(model.device), masks.to(model.device)            outputs = model(ct_slices)            preds = torch.argmax(outputs, dim=1)            confidences = torch.softmax(outputs, dim=1)[:, 1]  # Confidence for positive class            val_dice_score += dice_coefficient(preds, masks)                        all_preds.extend(preds.cpu().numpy().flatten())            all_masks.extend(masks.cpu().numpy().flatten())            all_confidences.extend(confidences.cpu().numpy().flatten())    val_dice_score /= len(val_loader)    print(f"Validation Dice Score: {val_dice_score:.4f}")    # Compute metrics    metrics = compute_metrics(all_preds, all_masks)    for key, value in metrics.items():        if key != 'iou_scores':            print(f"{key.capitalize()}: {value:.4f}")        print("IOU Scores:")    for threshold, iou in zip([0.5, 0.6, 0.7, 0.8, 0.9], metrics['iou_scores']):        print(f"  Threshold {threshold}: {iou:.4f}")    # Plot confusion matrix    cm = confusion_matrix(all_masks, all_preds)    plt.figure(figsize=(8, 6))    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')    plt.xlabel('Predicted')    plt.ylabel('True')    plt.title('Confusion Matrix')    plt.show()    # Box plot of metrics    plt.figure(figsize=(10, 6))    sns.boxplot(data=[metrics['accuracy'], metrics['sensitivity'], metrics['specificity'],                      metrics['precision'], metrics['recall'], metrics['f1_score'],                      metrics['dcs']] + metrics['iou_scores'])    plt.xticks(range(13), ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'Recall',                           'F1-score', 'DCS'] + [f'IOU-{t}' for t in [0.5, 0.6, 0.7, 0.8, 0.9]])    plt.title('Distribution of Metrics')    plt.show()    # Visualize some predictions with confidence scores    def visualize_prediction(ct_slice, true_mask, pred_mask, confidence):        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))        ax1.imshow(ct_slice.squeeze(), cmap='bone')        ax1.set_title('CT Slice')        ax1.axis('off')                ax2.imshow(ct_slice.squeeze(), cmap='bone')        ax2.imshow(true_mask, alpha=0.5, cmap='autumn')        ax2.set_title('True Mask')        ax2.axis('off')                ax3.imshow(ct_slice.squeeze(), cmap='bone')        ax3.imshow(pred_mask, alpha=0.5, cmap='autumn')        ax3.set_title(f'Predicted Mask\nConfidence: {confidence:.4f}')        ax3.axis('off')                plt.tight_layout()        plt.show()    # Visualize 4 random validation predictions with confidence scores    model.eval()    with torch.no_grad():        indices = random.sample(range(len(val_dataset)), 4)        for i in indices:            ct_slice, true_mask = val_dataset[i]            ct_slice = ct_slice.unsqueeze(0).to(model.device)            output = model(ct_slice)            pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()            confidence = torch.softmax(output, dim=1)[0, 1].item()  # Confidence for positive class                        visualize_prediction(ct_slice.cpu().squeeze(), true_mask.squeeze(), pred_mask, confidence)if __name__ == '__main__':    main()